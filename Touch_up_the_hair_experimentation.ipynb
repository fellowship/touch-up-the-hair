{"cells":[{"cell_type":"markdown","metadata":{"id":"m-Zck4hO5nlJ"},"source":["## Install  libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GhP5rvw6F8Q","outputId":"2d2e9dff-36c4-4e92-9e60-47badb066b5f","executionInfo":{"status":"ok","timestamp":1711676822331,"user_tz":300,"elapsed":695589,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m936.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cublas-cu12/\u001b[0m\u001b[33m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for controlnet_aux (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install mediapipe -q\n","!pip install --upgrade diffusers[torch] -q\n","!pip install transformers -q\n","!pip install accelerate -q\n","!pip install git+https://github.com/huggingface/diffusers -q\n","!pip install -qU controlnet_aux -q"]},{"cell_type":"markdown","metadata":{"id":"6RQZ7gDe6h6q"},"source":["# Clone CodeFormer"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWcEWIJ2xDnB","outputId":"3537015b-3ca1-407c-8c58-2bdedb93e9cf","executionInfo":{"status":"ok","timestamp":1711676910673,"user_tz":300,"elapsed":42479,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'CodeFormer'...\n","remote: Enumerating objects: 594, done.\u001b[K\n","remote: Counting objects: 100% (594/594), done.\u001b[K\n","remote: Compressing objects: 100% (316/316), done.\u001b[K\n","remote: Total 594 (delta 287), reused 493 (delta 269), pack-reused 0\u001b[K\n","Receiving objects: 100% (594/594), 17.31 MiB | 33.95 MiB/s, done.\n","Resolving deltas: 100% (287/287), done.\n","/content/CodeFormer\n","Collecting addict (from -r requirements.txt (line 1))\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.18.3)\n","Collecting lmdb (from -r requirements.txt (line 3))\n","  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.8.0.76)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (9.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.31.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.11.4)\n","Collecting tb-nightly (from -r requirements.txt (line 11))\n","  Downloading tb_nightly-2.17.0a20240328-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.17.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.66.2)\n","Collecting yapf (from -r requirements.txt (line 15))\n","  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lpips (from -r requirements.txt (line 16))\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.7.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2024.2.2)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (3.2.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2024.2.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 9)) (24.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (1.62.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 11)) (3.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->-r requirements.txt (line 12)) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->-r requirements.txt (line 12)) (12.4.99)\n","Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 15)) (7.1.0)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 15)) (4.2.0)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 15)) (2.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 17)) (4.12.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->-r requirements.txt (line 15)) (3.18.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 11)) (2.1.5)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 17)) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (1.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->-r requirements.txt (line 12)) (1.3.0)\n","Installing collected packages: lmdb, addict, yapf, tb-nightly, lpips\n","Successfully installed addict-2.4.0 lmdb-1.4.1 lpips-0.1.4 tb-nightly-2.17.0a20240328 yapf-0.40.2\n","/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Requirements should be satisfied by a PEP 517 installer.\n","        If you are using pip, you can try `pip install --use-pep517`.\n","        ********************************************************************************\n","\n","!!\n","  dist.fetch_build_eggs(dist.setup_requires)\n","running develop\n","/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` and ``easy_install``.\n","        Instead, use pypa/build, pypa/installer, pypa/build or\n","        other standards-based tools.\n","\n","        See https://github.com/pypa/setuptools/issues/917 for details.\n","        ********************************************************************************\n","\n","!!\n","  easy_install.initialize_options(self)\n","/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` directly.\n","        Instead, use pypa/build, pypa/installer, pypa/build or\n","        other standards-based tools.\n","\n","        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n","        ********************************************************************************\n","\n","!!\n","  self.initialize_options()\n","running egg_info\n","creating basicsr.egg-info\n","writing basicsr.egg-info/PKG-INFO\n","writing dependency_links to basicsr.egg-info/dependency_links.txt\n","writing requirements to basicsr.egg-info/requires.txt\n","writing top-level names to basicsr.egg-info/top_level.txt\n","writing manifest file 'basicsr.egg-info/SOURCES.txt'\n","reading manifest file 'basicsr.egg-info/SOURCES.txt'\n","adding license file 'LICENSE'\n","writing manifest file 'basicsr.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","running build_ext\n","Creating /usr/local/lib/python3.10/dist-packages/basicsr.egg-link (link to .)\n","Adding basicsr 1.3.2 to easy-install.pth file\n","\n","Installed /content/CodeFormer\n","Processing dependencies for basicsr==1.3.2\n","Searching for gdown==4.7.3\n","Best match: gdown 4.7.3\n","Adding gdown 4.7.3 to easy-install.pth file\n","Installing gdown script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for lpips==0.1.4\n","Best match: lpips 0.1.4\n","Adding lpips 0.1.4 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for yapf==0.40.2\n","Best match: yapf 0.40.2\n","Adding yapf 0.40.2 to easy-install.pth file\n","Installing yapf script to /usr/local/bin\n","Installing yapf-diff script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for tqdm==4.66.2\n","Best match: tqdm 4.66.2\n","Adding tqdm 4.66.2 to easy-install.pth file\n","Installing tqdm script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for torchvision==0.17.1+cu121\n","Best match: torchvision 0.17.1+cu121\n","Adding torchvision 0.17.1+cu121 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for torch==2.2.1+cu121\n","Best match: torch 2.2.1+cu121\n","Adding torch 2.2.1+cu121 to easy-install.pth file\n","Installing convert-caffe2-to-onnx script to /usr/local/bin\n","Installing convert-onnx-to-caffe2 script to /usr/local/bin\n","Installing torchrun script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for tb-nightly==2.17.0a20240328\n","Best match: tb-nightly 2.17.0a20240328\n","Adding tb-nightly 2.17.0a20240328 to easy-install.pth file\n","Installing tensorboard script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for scipy==1.11.4\n","Best match: scipy 1.11.4\n","Adding scipy 1.11.4 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for scikit-image==0.19.3\n","Best match: scikit-image 0.19.3\n","Adding scikit-image 0.19.3 to easy-install.pth file\n","Installing skivi script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for requests==2.31.0\n","Best match: requests 2.31.0\n","Adding requests 2.31.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for PyYAML==6.0.1\n","Best match: PyYAML 6.0.1\n","Adding PyYAML 6.0.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for Pillow==9.4.0\n","Best match: Pillow 9.4.0\n","Adding Pillow 9.4.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for opencv-python==4.8.0.76\n","Best match: opencv-python 4.8.0.76\n","Adding opencv-python 4.8.0.76 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for numpy==1.25.2\n","Best match: numpy 1.25.2\n","Adding numpy 1.25.2 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.10 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for lmdb==1.4.1\n","Best match: lmdb 1.4.1\n","Adding lmdb 1.4.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for future==0.18.3\n","Best match: future 0.18.3\n","Adding future 0.18.3 to easy-install.pth file\n","Installing futurize script to /usr/local/bin\n","Installing pasteurize script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for addict==2.4.0\n","Best match: addict 2.4.0\n","Adding addict 2.4.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for beautifulsoup4==4.12.3\n","Best match: beautifulsoup4 4.12.3\n","Adding beautifulsoup4 4.12.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for six==1.16.0\n","Best match: six 1.16.0\n","Adding six 1.16.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for filelock==3.13.3\n","Best match: filelock 3.13.3\n","Adding filelock 3.13.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for tomli==2.0.1\n","Best match: tomli 2.0.1\n","Adding tomli 2.0.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for platformdirs==4.2.0\n","Best match: platformdirs 4.2.0\n","Adding platformdirs 4.2.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for importlib-metadata==7.1.0\n","Best match: importlib-metadata 7.1.0\n","Adding importlib-metadata 7.1.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for triton==2.2.0\n","Best match: triton 2.2.0\n","Adding triton 2.2.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-nvtx-cu12==12.1.105\n","Best match: nvidia-nvtx-cu12 12.1.105\n","Adding nvidia-nvtx-cu12 12.1.105 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-nccl-cu12==2.19.3\n","Best match: nvidia-nccl-cu12 2.19.3\n","Adding nvidia-nccl-cu12 2.19.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cusparse-cu12==12.1.0.106\n","Best match: nvidia-cusparse-cu12 12.1.0.106\n","Adding nvidia-cusparse-cu12 12.1.0.106 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cusolver-cu12==11.4.5.107\n","Best match: nvidia-cusolver-cu12 11.4.5.107\n","Adding nvidia-cusolver-cu12 11.4.5.107 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-curand-cu12==10.3.2.106\n","Best match: nvidia-curand-cu12 10.3.2.106\n","Adding nvidia-curand-cu12 10.3.2.106 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cufft-cu12==11.0.2.54\n","Best match: nvidia-cufft-cu12 11.0.2.54\n","Adding nvidia-cufft-cu12 11.0.2.54 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cublas-cu12==12.1.3.1\n","Best match: nvidia-cublas-cu12 12.1.3.1\n","Adding nvidia-cublas-cu12 12.1.3.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cudnn-cu12==8.9.2.26\n","Best match: nvidia-cudnn-cu12 8.9.2.26\n","Adding nvidia-cudnn-cu12 8.9.2.26 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cuda-cupti-cu12==12.1.105\n","Best match: nvidia-cuda-cupti-cu12 12.1.105\n","Adding nvidia-cuda-cupti-cu12 12.1.105 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cuda-runtime-cu12==12.1.105\n","Best match: nvidia-cuda-runtime-cu12 12.1.105\n","Adding nvidia-cuda-runtime-cu12 12.1.105 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-cuda-nvrtc-cu12==12.1.105\n","Best match: nvidia-cuda-nvrtc-cu12 12.1.105\n","Adding nvidia-cuda-nvrtc-cu12 12.1.105 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for fsspec==2023.6.0\n","Best match: fsspec 2023.6.0\n","Adding fsspec 2023.6.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for Jinja2==3.1.3\n","Best match: Jinja2 3.1.3\n","Adding Jinja2 3.1.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for networkx==3.2.1\n","Best match: networkx 3.2.1\n","Adding networkx 3.2.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for sympy==1.12\n","Best match: sympy 1.12\n","Adding sympy 1.12 to easy-install.pth file\n","Installing isympy script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for typing-extensions==4.10.0\n","Best match: typing-extensions 4.10.0\n","Adding typing-extensions 4.10.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for werkzeug==3.0.1\n","Best match: werkzeug 3.0.1\n","Adding werkzeug 3.0.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for tensorboard-data-server==0.7.2\n","Best match: tensorboard-data-server 0.7.2\n","Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for setuptools==67.7.2\n","Best match: setuptools 67.7.2\n","Adding setuptools 67.7.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for protobuf==3.20.3\n","Best match: protobuf 3.20.3\n","Adding protobuf 3.20.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for Markdown==3.6\n","Best match: Markdown 3.6\n","Adding Markdown 3.6 to easy-install.pth file\n","Installing markdown_py script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for grpcio==1.62.1\n","Best match: grpcio 1.62.1\n","Adding grpcio 1.62.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for absl-py==1.4.0\n","Best match: absl-py 1.4.0\n","Adding absl-py 1.4.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for packaging==24.0\n","Best match: packaging 24.0\n","Adding packaging 24.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for pywavelets==1.5.0\n","Best match: pywavelets 1.5.0\n","Adding pywavelets 1.5.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for tifffile==2024.2.12\n","Best match: tifffile 2024.2.12\n","Adding tifffile 2024.2.12 to easy-install.pth file\n","Installing lsm2bin script to /usr/local/bin\n","Installing tiff2fsspec script to /usr/local/bin\n","Installing tiffcomment script to /usr/local/bin\n","Installing tifffile script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for imageio==2.31.6\n","Best match: imageio 2.31.6\n","Adding imageio 2.31.6 to easy-install.pth file\n","Installing imageio_download_bin script to /usr/local/bin\n","Installing imageio_remove_bin script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for certifi==2024.2.2\n","Best match: certifi 2024.2.2\n","Adding certifi 2024.2.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for urllib3==2.0.7\n","Best match: urllib3 2.0.7\n","Adding urllib3 2.0.7 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for idna==3.6\n","Best match: idna 3.6\n","Adding idna 3.6 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for charset-normalizer==3.3.2\n","Best match: charset-normalizer 3.3.2\n","Adding charset-normalizer 3.3.2 to easy-install.pth file\n","Installing normalizer script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for soupsieve==2.5\n","Best match: soupsieve 2.5\n","Adding soupsieve 2.5 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for PySocks==1.7.1\n","Best match: PySocks 1.7.1\n","Adding PySocks 1.7.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for zipp==3.18.1\n","Best match: zipp 3.18.1\n","Adding zipp 3.18.1 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for nvidia-nvjitlink-cu12==12.4.99\n","Best match: nvidia-nvjitlink-cu12 12.4.99\n","Adding nvidia-nvjitlink-cu12 12.4.99 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for MarkupSafe==2.1.5\n","Best match: MarkupSafe 2.1.5\n","Adding MarkupSafe 2.1.5 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for mpmath==1.3.0\n","Best match: mpmath 1.3.0\n","Adding mpmath 1.3.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Finished processing dependencies for basicsr==1.3.2\n","Downloading: \"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/detection_Resnet50_Final.pth\" to /content/CodeFormer/weights/facelib/detection_Resnet50_Final.pth\n","\n","100% 104M/104M [00:01<00:00, 96.2MB/s] \n","Downloading: \"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/parsing_parsenet.pth\" to /content/CodeFormer/weights/facelib/parsing_parsenet.pth\n","\n","100% 81.4M/81.4M [00:00<00:00, 174MB/s]\n","Downloading: \"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth\" to /content/CodeFormer/weights/CodeFormer/codeformer.pth\n","\n","100% 359M/359M [00:01<00:00, 232MB/s]\n","/content\n"]}],"source":["# Clone CodeFormer and enter the CodeFormer folder\n","%cd /content/\n","!rm -rf CodeFormer\n","!git clone https://github.com/sczhou/CodeFormer.git\n","%cd CodeFormer\n","\n","# Set up the environment\n","# Install python dependencies\n","!pip install -r requirements.txt\n","# Install basicsr\n","!python basicsr/setup.py develop\n","\n","# Download the pre-trained model\n","!python scripts/download_pretrained_models.py facelib\n","!python scripts/download_pretrained_models.py CodeFormer\n","%cd /content/"]},{"cell_type":"markdown","metadata":{"id":"JxPaPvK0AKLB"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"m9zF4ctRzfoq","executionInfo":{"status":"ok","timestamp":1711691603317,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","import cv2\n","import math\n","import numpy as np\n","from PIL import Image\n","from numpy import float32\n","import matplotlib.pyplot as plt\n","import requests\n","import torch\n","import PIL\n","from diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline, DPMSolverMultistepScheduler,UniPCMultistepScheduler,EulerDiscreteScheduler\n","from diffusers.utils import load_image\n","import torch\n","from google.colab import userdata\n","import io\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","import os\n","import shutil\n","import logging\n","from pathlib import Path\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"R5O_FQFN7ZKR"},"source":["# Mount google drive"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOissoN373vX","outputId":"b2ca03b3-63bb-4eb4-af89-abe5154f20d7","executionInfo":{"status":"ok","timestamp":1711686987376,"user_tz":300,"elapsed":520,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"pI2NtI1yJl1h","executionInfo":{"status":"ok","timestamp":1711686987376,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["def face_restoration(image_in):\n","    %cd /content/CodeFormer/\n","    upload_folder = 'inputs/user_upload'\n","    if os.path.isdir(upload_folder):\n","        shutil.rmtree(upload_folder)\n","    os.mkdir(upload_folder)\n","    image_in.save(os.path.join(upload_folder,'image.png'))\n","\n","    # We set w to 0.7 for the whole images\n","    # you can add '--bg_upsampler realesrgan' to enhance the background\n","    #@markdown *Codeformer_Fidelity (0 for better quality, 1 for better identity)*\n","    CODEFORMER_FEDILITY = 0.7 # @param {type:\"slider\", min:0, max:1, step:0.05}\n","    !python inference_codeformer.py -w $CODEFORMER_FEDILITY --input_path inputs/user_upload --bg_upsampler realesrgan\n","    %cd /content/\n","    img_out = Image.open(f'/content/CodeFormer/results/user_upload_{CODEFORMER_FEDILITY}/final_results/image.png')\n","    return img_out\n"]},{"cell_type":"markdown","metadata":{"id":"ehA4rRXVILdo"},"source":["# Create Required Directories and Upload Images\n","\n","1.   List item\n","2.   List item\n","\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"sabeMdUMmlYt","executionInfo":{"status":"ok","timestamp":1711686989665,"user_tz":300,"elapsed":103,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["project_dir_path = '/content/drive/MyDrive/touch-up-the-hair'\n","minority_color_mask_dir_path = 'output_dir/minority_color_masks'\n","majority_color_mask_dir_path = 'output_dir/majority_color_masks'\n","touched_dir_path = 'output_dir/touched'\n","restored_dir_path = 'output_dir/touched_and_restored'\n","directory_list = ['models', 'input_dir', minority_color_mask_dir_path, majority_color_mask_dir_path, touched_dir_path, restored_dir_path]\n","for directory in directory_list:\n","    directory_path = os.path.join(project_dir_path, directory)\n","    if not os.path.exists(directory_path):\n","        os.makedirs(directory_path)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"9wIpOtJs_guz","executionInfo":{"status":"ok","timestamp":1711686989914,"user_tz":300,"elapsed":2,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["selfie_multiclass_model_path = os.path.join(project_dir_path, \"models\", \"selfie_multiclass_256x256.tflite\")\n","selfie_multiclass_model_url = \"https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite\"\n","if(not os.path.exists(selfie_multiclass_model_path)):\n","  # Download the model\n","  urllib.request.urlretrieve(selfie_multiclass_model_url,selfie_multiclass_model_path)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"g0YodbHBxQDf","executionInfo":{"status":"ok","timestamp":1711686990198,"user_tz":300,"elapsed":1,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["#@markdown # **Generate Class**\n","class GenerateMask(object):\n","\n","    def __init__(self, image_path):\n","\n","        self._image_BGR = cv2.imread(image_path)\n","        if self._image_BGR is None:\n","            raise ValueError(f\"Failed to load image from path: {image_path}\")\n","\n","        # Convert image to RGB (OpenCV uses BGR by default)\n","        self._image_RGB = cv2.cvtColor(self._image_BGR, cv2.COLOR_BGR2RGB)\n","\n","        # Model path\n","        self._HAIR_SEGMENTER_MODEL_PATH = \"/content/drive/MyDrive/touch-up-the-hair/models/selfie_multiclass_256x256.tflite\"\n","\n","        # Create the options for ImageSegmenter\n","        base_options = python.BaseOptions(model_asset_path = self._HAIR_SEGMENTER_MODEL_PATH)\n","        self._options = vision.ImageSegmenterOptions(base_options = base_options, output_category_mask = True)\n","\n","        self.MASK_COLOR = (255, 255, 255)  # Define MASK_COLOR\n","        self.BG_COLOR = (0, 0, 0)  # Define BG_COLOR\n","\n","    def get_image_BGR(self):\n","        \"\"\"\n","            return image_BGR\n","        \"\"\"\n","        return self._image_BGR\n","\n","    def get_image_RGB(self):\n","        \"\"\"\n","            return image_RGB\n","        \"\"\"\n","        return self._image_RGB\n","\n","\n","    def get_mask(self,CLASS_INDEX = 1):\n","        \"\"\"\n","            Get the hair mask using Mediapipe's ImageSegmenter.\n","        Returns:\n","            - np.array: Hair mask image.\n","        \"\"\"\n","\n","        # Convert the image to Mediapipe's format\n","        image_mediapipe = mp.Image(image_format = mp.ImageFormat.SRGB, data = self._image_BGR)\n","\n","        # Retrieve the masks for the segmented image\n","        with vision.ImageSegmenter.create_from_options(self._options) as segmenter:\n","            segmentation_result = segmenter.segment(image_mediapipe)\n","            category_mask = segmentation_result.category_mask.numpy_view()\n","\n","            mask_image = np.zeros(self._image_BGR.shape, dtype = np.uint8)\n","            bg_image = mask_image.copy()\n","            mask_image[:] = self.MASK_COLOR\n","            bg_image[:] = self.BG_COLOR\n","\n","            # Generate hair mask\n","            condition = np.stack((category_mask,) * 3, axis = -1) == CLASS_INDEX\n","            mask = np.where(condition, mask_image, bg_image)\n","\n","            return mask\n","\n","    def get_hair_mask(self):\n","        return self.get_mask()\n","\n","    def get_face_mask(self):\n","        return self.get_mask(CLASS_INDEX = 3)\n","\n","\n","    def get_gray_mask(self,mask_path = None):\n","\n","        if mask_path is None:\n","            # If hair_mask_path is not available, calculate hair mask on the fly\n","            mask = self.get_hair_mask()\n","            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n","        else:\n","            # Load the binary mask\n","            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","        return mask\n","\n","    def get_minority_hair_mask(self, mask_path = None, num_cluster = 2, model = \"KMeans\", majority=False):\n","        \"\"\"\n","        Generate a hair minority mask based on the input image and its corresponding mask.\n","\n","        Parameters:\n","        - image_path (str): Path to the input image.\n","        - mask_path (str): Path to the binary mask where hair regions are white and background is black.\n","        - num_cluster (int): Number of clusters for KMeans clustering (default is 3).\n","        - model (str): Model to use for clustering (default is \"KMeans\").\n","        - majority (bool): Whether to use the majority cluster (default is False).\n","        Returns:\n","        - np.array: Hair minority mask image.\n","        \"\"\"\n","\n","        mask = self.get_gray_mask(mask_path = mask_path)\n","\n","        #Perform K-means clustering solely on hair pixels,\n","        #creating two clusters instead of analyzing the entire image with three clusters.\n","        #This approach is chosen to avoid potential clustering of\n","        #dark black hair with the background due to similar color tones.\n","        #Subsequently, identify non-zero pixels within the mask, representing the masked area.\n","        mask_indices = np.where(mask > 0)\n","\n","        # Get the pixels within the masked area\n","        pixels = self._image_RGB[mask_indices].reshape((-1, 3))\n","\n","        # Perform KMeans clustering on the pixels\n","        if model == \"KMeans\":\n","          model = KMeans(n_clusters = num_cluster, n_init = 10)\n","        elif model == \"GaussianMixture\":\n","          model = GaussianMixture(n_components = num_cluster, covariance_type = 'full')\n","        else:\n","          raise ValueError(\"Invalid model name. Valid options are 'KMeans' and 'GaussianMixture'.\")\n","\n","        model.fit(pixels)\n","\n","        # Get the labels for each pixel\n","        labels = model.predict(pixels)\n","\n","        # Find the cluster with the fewest pixels\n","        if majority == False:\n","          min_cluster_label = np.bincount(labels).argmin()\n","        else:\n","          min_cluster_label = np.bincount(labels).argmax()\n","\n","        # Create a binary mask where pixels belonging to the minimum cluster class are white\n","        cluster_mask = np.zeros_like(mask)\n","        cluster_mask[mask_indices] = 255 * (labels == min_cluster_label)\n","\n","        rgb_img_hair_minority_mask = cv2.cvtColor(cluster_mask, cv2.COLOR_BGR2RGB)\n","\n","        return rgb_img_hair_minority_mask\n","\n","\n","    def get_gray_minority_mask(self, minority_mask_path = None):\n","        if minority_mask_path is None:\n","            # If hair_mask_path is not available, calculate hair mask on the fly\n","            minority_mask = self.get_minority_hair_mask()\n","            minority_mask = cv2.cvtColor(minority_mask, cv2.COLOR_BGR2GRAY)\n","        else:\n","            # Load the binary mask\n","            minority_mask = cv2.imread(minority_mask_path, cv2.IMREAD_GRAYSCALE)\n","        return minority_mask\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"DgSaZvgv-rZt","executionInfo":{"status":"ok","timestamp":1711686991534,"user_tz":300,"elapsed":96,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["# #@markdown # **Stable Diffusion Controlnet Pipeline Class**\n","# class StableDiffusionControlnetPipeline(object):\n","#     def __init__(self):\n","#         self.MODEL_PATHS = {\n","#             \"RealVisInpaint\": \"Uminosachi/realisticVisionV51_v51VAE-inpainting\"\n","#         }\n","#         self.CONTROLNET_PATHS = {\n","#             \"ControlNetInpaint\": \"lllyasviel/control_v11p_sd15_inpaint\",\n","#             \"ControlNetCanny\": \"lllyasviel/control_v11p_sd15_canny\",\n","#         }\n","\n","#         self.schedulers = {\n","#             \"UniPCMultistepScheduler\": UniPCMultistepScheduler,\n","#             \"DPMSolverMultistepScheduler\": DPMSolverMultistepScheduler,\n","#             \"EulerDiscreteScheduler\": EulerDiscreteScheduler\n","#         }\n","\n","#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","#         #Loading the ControlNet Inpaint and Canny Models\n","#         self.controlnet = [\n","#             ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=torch.float16).to(self.device) for _, CONTROLNET_PATH in self.CONTROLNET_PATHS.items()\n","#         ]\n","#         controlnet_inpaint_scale = 0.5 # @param {type:\"number\"}\n","#         controlnet_canny_scale = 0.2 # @param {type:\"number\"}\n","#         self.controlnet_scales = [controlnet_inpaint_scale, controlnet_canny_scale]\n","\n","\n","#         assert len(self.controlnet) == len(self.controlnet_scales)\n","\n","#         #Loading the Stabel Difussion RealisticVision ControlNet Inpaint Pipeline\n","#         pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n","#             self.MODEL_PATHS[\"RealVisInpaint\"],\n","#             controlnet=self.controlnet,\n","#             safety_checker=None,\n","#             requires_safety_checker=False,\n","#             torch_dtype=torch.float16,\n","#         ).to(self.device)\n","#         pipe.scheduler = self.schedulers[\"UniPCMultistepScheduler\"].from_config(pipe.scheduler.config)\n","#         self.pipe = pipe\n","\n","#     def make_inpaint_condition(self, image, image_mask):\n","#         image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n","#         image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n","#         assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n","#         image[image_mask > 0.5] = -1.0  # set as masked pixel\n","#         image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n","#         image = torch.from_numpy(image)\n","#         return image\n","\n","#     def make_canny_condition(self, image):\n","#         image = np.array(image)\n","#         image = cv2.Canny(image, 100, 200)\n","#         image = image[:, :, None]\n","#         image = np.concatenate([image, image, image], axis=2)\n","#         image = Image.fromarray(image)\n","#         return image\n","\n","#     def roundUp(self, input, round):\n","#         return input + round - (input % round)\n","\n","#     # Function to Edit the Hair Roots of the Img with Stable Diffusion and ControlNet\n","#     def stable_diffusion_controlnet(self, image_path, PROMPT, NEGATIVE_PROMPT, minority_mask_path = None):\n","#         pillow_img = Image.open(image_path)\n","#         cv2_img = np.asarray(pillow_img)\n","#         if minority_mask_path is None:\n","#           # If hair_mask_path is not available, calculate hair mask on the fly\n","#           mask_generator = GenerateMask(image_path)\n","#           HAIR_ROOT_MASK = mask_generator.get_minority_hair_mask()\n","#         else:\n","#           # Load the binary mask\n","#             HAIR_ROOT_MASK = cv2.imread(minority_mask_path)\n","\n","#         HAIR_ROOT_MASK = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2GRAY)\n","\n","#         kernel = np.ones((12, 12), np.uint8)#expansion gives better results\n","#         HAIR_ROOT_MASK = cv2.dilate(HAIR_ROOT_MASK, kernel, iterations=1)\n","\n","#         mask_image = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2RGB)\n","#         mask_image = Image.fromarray(mask_image)\n","\n","#         # Resize the image to be divisible by 8\n","#         height = self.roundUp(pillow_img.height, 8)\n","#         width = self.roundUp(pillow_img.width, 8)\n","#         SEED = 42 # @param {type:\"integer\"}\n","#         num_inference_steps = 50 # @param {type:\"integer\"},\n","#         strength = 1.0 # @param {type:\"number\"},\n","#         guidance_scale = 4.0 # @param {type:\"number\"}\n","\n","#         generator = torch.Generator(device=self.device).manual_seed(SEED)\n","\n","#         inpaint_condition = self.make_inpaint_condition(pillow_img, mask_image)\n","#         canny_condition = self.make_canny_condition(pillow_img)\n","#         control_images = [inpaint_condition, canny_condition]\n","#         touched_image = self.pipe(\n","#             prompt = PROMPT,\n","#             image = pillow_img,\n","#             mask_image = mask_image,\n","#             num_inference_steps = num_inference_steps,\n","#             generator = generator,\n","#             control_image=control_images,\n","#             controlnet_conditioning_scale = self.controlnet_scales,\n","#             negative_prompt = NEGATIVE_PROMPT,\n","#             strength = strength,\n","#             height = height,\n","#             width = width,\n","#             guidance_scale = guidance_scale,\n","\n","#         ).images[0]\n","\n","\n","#         return HAIR_ROOT_MASK, touched_image\n","\n","#     def loss(image, min_color_mask, maj_color_mask):\n","#       \"\"\"\n","#       This function computes the Wasserstein distance (aka Earth Mover's Distance) between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n","\n","#       In theory, having the touched up region's pixel color distribution appear as similar as possible to that of the majority color region is desirable because\n","#       the touched up region will blend in better with the remainder of the hair. This improves the color while preserving more \"naturalness\".  Natural hair texture is often lost because it\n","#       appears that the hair texture in an image is numerically represented by particular patterns of minor variations in RGB values (the RGB values of which are approximated by grayscale pixel intensity)\n","#       and that these patters are vulnerable to being \"smoothed\" out of existence by the Stable Diffusion model.  Attempting to optimize this loss function is attempting to minimize the amount of\n","#       \"smoothness\" afflicting the touched up region while pushing the mean (and median) pixel color in the direction of the majority color region.\n","\n","\n","#       Parameters:\n","#       image (numpy array): The original image.\n","#       min_color_mask (numpy array): The (grayscale) minority color segmentation mask.\n","#       maj_color_mask (numpy array): The (grayscale) majority color segmentation mask.\n","\n","#       Returns:\n","#       loss (float): The Wasserstein distance between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n","#       \"\"\"\n","\n","#       # Create black background image equal in size to the original image\n","#       colored_min_region = np.zeros_like(image)\n","#       colored_maj_region = np.zeros_like(image)\n","\n","#       # Extract the indices of the pixel regions of each mask\n","#       min_mask_regions = np.where(min_color_mask > 0)\n","#       maj_mask_regions = np.where(maj_color_mask > 0)\n","\n","#       # Map the colored pixels from the minority and majority color segmentation masks onto the black background image\n","#       colored_min_region[min_mask_regions] = image[min_mask_regions]\n","#       colored_maj_region[maj_mask_regions] = image[maj_mask_regions]\n","\n","#       \"\"\"\n","#       The two lines below convert RGB images to grayscale images.\n","#       NOTE: The formula to do, based on relative perception of color brightness, is the following: 0.299 ∙ Red + 0.587 ∙ Green + 0.114 ∙ Blue\n","#       The corresponding grayscale pixel value is a weighted sum of the RGB pixel values and represents the pixel \"intensity\".\n","#       \"\"\"\n","#       gray_min_region = cv2.cvtColor(colored_min_region, cv2.COLOR_BGR2GRAY)\n","#       gray_maj_region = cv2.cvtColor(colored_maj_region, cv2.COLOR_BGR2GRAY)\n","\n","#       # Flatten each greyscale image to a vector\n","#       gray_min_vector = gray_min_region.flatten()\n","#       gray_maj_vector = gray_maj_region.flatten()\n","\n","#       # Extract nonzero pixel values (ie discard the black background pixels)\n","#       gray_min_pixels = gray_min_vector[gray_min_vector > 0]\n","#       gray_maj_pixels = gray_maj_vector[gray_maj_vector > 0]\n","\n","#       # Compute histogram of pixel intensities for the minority color segmentation mask\n","#       min_mask_distribution = np.bincount(gray_min_pixels) / gray_min_pixels.size\n","\n","#       # Compute histogram of pixel intensities for the majority color segmentation mask\n","#       maj_mask_distribution = np.bincount(gray_maj_pixels) / gray_maj_pixels.size\n","\n","#       # Ensure that the probability distribtion sums to 1\n","#       assert(min_mask_distribution.sum() == 1)\n","#       assert(maj_mask_distribution.sum() == 1)\n","\n","#       # Compute Wasserstein distance between the two histograms of pixel intensities\n","#       distribution_distance = scipy.stats.wasserstein_distance(min_mask_distribution, maj_mask_distribution, u_weights=None, v_weights=None)\n","#       loss = distribution_distance\n","\n","#       return loss\n","\n"]},{"cell_type":"code","source":["class StableDiffusionControlnetPipeline(object):\n","    def __init__(self):\n","        self.MODEL_PATHS = {\n","            \"RealVisInpaint\": \"Uminosachi/realisticVisionV51_v51VAE-inpainting\"\n","        }\n","        self.CONTROLNET_PATHS = {\n","            \"ControlNetInpaint\": \"lllyasviel/control_v11p_sd15_inpaint\",\n","            \"ControlNetCanny\": \"lllyasviel/control_v11p_sd15_canny\",\n","        }\n","\n","        self.schedulers = {\n","            \"UniPCMultistepScheduler\": UniPCMultistepScheduler,\n","            \"DPMSolverMultistepScheduler\": DPMSolverMultistepScheduler,\n","            \"EulerDiscreteScheduler\": EulerDiscreteScheduler\n","        }\n","\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        #Loading the ControlNet Inpaint and Canny Models\n","        self.controlnet = [\n","            ControlNetModel.from_pretrained(CONTROLNET_PATH, torch_dtype=torch.float16).to(self.device) for _, CONTROLNET_PATH in self.CONTROLNET_PATHS.items()\n","        ]\n","\n","        #assert len(self.controlnet) == len(self.controlnet_scales)\n","\n","        #Loading the Stable Diffusion RealisticVision ControlNet Inpaint Pipeline\n","        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n","            self.MODEL_PATHS[\"RealVisInpaint\"],\n","            controlnet=self.controlnet,\n","            safety_checker=None,\n","            requires_safety_checker=False,\n","            torch_dtype=torch.float16,\n","        ).to(self.device)\n","        pipe.scheduler = self.schedulers[\"UniPCMultistepScheduler\"].from_config(pipe.scheduler.config)\n","        self.pipe = pipe\n","\n","\n","    def make_inpaint_condition(self, image, image_mask):\n","        image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n","        image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n","        assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n","        image[image_mask > 0.5] = -1.0  # set as masked pixel\n","        image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n","        image = torch.from_numpy(image)\n","        return image\n","\n","    def make_canny_condition(self, image):\n","        image = np.array(image)\n","        image = cv2.Canny(image, 100, 200)\n","        image = image[:, :, None]\n","        image = np.concatenate([image, image, image], axis=2)\n","        image = Image.fromarray(image)\n","        return image\n","\n","    def roundUp(self, input, round):\n","        return input + round - (input % round)\n","\n","    # Function to dilate the mask (used in production only, NOT for generating minority and majority mask inputs to the loss function!!!)\n","    def dilate_mask(self, hair_root_mask):\n","        kernel = np.ones((12, 12), np.uint8)#expansion gives better results\n","        HAIR_ROOT_MASK = cv2.dilate(HAIR_ROOT_MASK, kernel, iterations=1)\n","\n","        return HAIR_ROOT_MASK\n","\n","    def get_mask(self, image_path, minority_mask_path = None):\n","        pillow_img = Image.open(image_path)\n","        cv2_img = np.asarray(pillow_img)\n","        if minority_mask_path is None:\n","          # If hair_mask_path is not available, calculate hair mask on the fly\n","          mask_generator = GenerateMask(image_path)\n","          HAIR_ROOT_MASK = mask_generator.get_minority_hair_mask()\n","        else:\n","          # Load the binary mask\n","            HAIR_ROOT_MASK = cv2.imread(minority_mask_path)\n","\n","        HAIR_ROOT_MASK = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2GRAY)\n","\n","        mask_image = cv2.cvtColor(HAIR_ROOT_MASK, cv2.COLOR_BGR2RGB)\n","        mask_image = Image.fromarray(mask_image)\n","\n","        return mask_image\n","\n","    # Function to Edit the Hair Roots of the Img with Stable Diffusion and ControlNet\n","    def stable_diffusion_controlnet(self, image_path, mask_image, PROMPT, NEGATIVE_PROMPT, dilate=False, num_inference_steps=50, strength=1.0, guidance_scale=4.0, controlnet_inpaint_scale=0.5, controlnet_canny_scale=0.2):\n","        pillow_img = Image.open(image_path)\n","\n","        \"\"\"\n","        Dilate the (minority color) mask if the condition is True; this condition is 'False' by default\n","        because we do not want to dilate the mask before passing it to the loss function (when we are tuning parameters).\n","        However, we do want to dilate the mask when producing a touched up image for the production pipeline\n","        (after a satisfactory set of parameters has been chosed).\n","        \"\"\"\n","\n","        if dilate == True:\n","          mask_image = self.dilate_mask(mask_image)\n","\n","        # Resize the image to be divisible by 8\n","        height = self.roundUp(pillow_img.height, 8)\n","        width = self.roundUp(pillow_img.width, 8)\n","\n","        SEED = 42\n","        generator = torch.Generator(device=self.device).manual_seed(SEED)\n","\n","        inpaint_condition = self.make_inpaint_condition(pillow_img, mask_image)\n","        canny_condition = self.make_canny_condition(pillow_img)\n","        control_images = [inpaint_condition, canny_condition]\n","        touched_image = self.pipe(\n","            prompt = PROMPT,\n","            image = pillow_img,\n","            mask_image = mask_image,\n","            num_inference_steps = num_inference_steps,\n","            generator = generator,\n","            control_image=control_images,\n","            controlnet_conditioning_scale = self.controlnet_scales,\n","            negative_prompt = NEGATIVE_PROMPT,\n","            strength = strength,\n","            height = height,\n","            width = width,\n","            guidance_scale = guidance_scale,\n","\n","        ).images[0]\n","\n","        return touched_image\n","\n","    def loss(image, min_color_mask, maj_color_mask):\n","      \"\"\"\n","      This function computes the Wasserstein distance (aka Earth Mover's Distance) between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n","\n","      In theory, having the touched up region's pixel color distribution appear as similar as possible to that of the majority color region is desirable because\n","      the touched up region will blend in better with the remainder of the hair. This improves the color while preserving more \"naturalness\".  Natural hair texture is often lost because it\n","      appears that the hair texture in an image is numerically represented by particular patterns of minor variations in RGB values (the RGB values of which are approximated by grayscale pixel intensity)\n","      and that these patters are vulnerable to being \"smoothed\" out of existence by the Stable Diffusion model.  Attempting to optimize this loss function is attempting to minimize the amount of\n","      \"smoothness\" afflicting the touched up region while pushing the mean (and median) pixel color in the direction of the majority color region.\n","\n","\n","      Parameters:\n","      image (numpy array): The original image.\n","      min_color_mask (numpy array): The (grayscale) minority color segmentation mask.\n","      maj_color_mask (numpy array): The (grayscale) majority color segmentation mask.\n","\n","      Returns:\n","      loss (float): The Wasserstein distance between the probability distributions of (grayscale) pixel intentsities between the minority and majority color segmentation masks.\n","      \"\"\"\n","\n","      # Create black background image equal in size to the original image\n","      colored_min_region = np.zeros_like(image)\n","      colored_maj_region = np.zeros_like(image)\n","\n","      # Extract the indices of the pixel regions of each mask\n","      min_mask_regions = np.where(min_color_mask > 0)\n","      maj_mask_regions = np.where(maj_color_mask > 0)\n","\n","      # Map the colored pixels from the minority and majority color segmentation masks onto the black background image\n","      colored_min_region[min_mask_regions] = image[min_mask_regions]\n","      colored_maj_region[maj_mask_regions] = image[maj_mask_regions]\n","\n","      \"\"\"\n","      The two lines below convert RGB images to grayscale images.\n","      NOTE: The formula to do, based on relative perception of color brightness, is the following: 0.299 ∙ Red + 0.587 ∙ Green + 0.114 ∙ Blue\n","      The corresponding grayscale pixel value is a weighted sum of the RGB pixel values and represents the pixel \"intensity\".\n","      \"\"\"\n","      gray_min_region = cv2.cvtColor(colored_min_region, cv2.COLOR_BGR2GRAY)\n","      gray_maj_region = cv2.cvtColor(colored_maj_region, cv2.COLOR_BGR2GRAY)\n","\n","      # Flatten each greyscale image to a vector\n","      gray_min_vector = gray_min_region.flatten()\n","      gray_maj_vector = gray_maj_region.flatten()\n","\n","      # Extract nonzero pixel values (ie discard the black background pixels)\n","      gray_min_pixels = gray_min_vector[gray_min_vector > 0]\n","      gray_maj_pixels = gray_maj_vector[gray_maj_vector > 0]\n","\n","      # Compute histogram of pixel intensities for the minority color segmentation mask\n","      min_mask_distribution = np.bincount(gray_min_pixels) / gray_min_pixels.size\n","\n","      # Compute histogram of pixel intensities for the majority color segmentation mask\n","      maj_mask_distribution = np.bincount(gray_maj_pixels) / gray_maj_pixels.size\n","\n","      # Ensure that the probability distribtion sums to 1\n","      assert(min_mask_distribution.sum() == 1)\n","      assert(maj_mask_distribution.sum() == 1)\n","\n","      # Compute Wasserstein distance between the two histograms of pixel intensities\n","      distribution_distance = scipy.stats.wasserstein_distance(min_mask_distribution, maj_mask_distribution, u_weights=None, v_weights=None)\n","      loss = distribution_distance\n","\n","      return loss\n","\n"],"metadata":{"id":"ceV5tnVucedP","executionInfo":{"status":"ok","timestamp":1711687030523,"user_tz":300,"elapsed":129,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jrn3lBm_ntnb"},"source":["# Instantiate the pipeline"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["80a67081ba3146eda7e386f76c375477","46959835013e4ff888fa8c980bb63b2f","272e61cde96447abad606cbba9781c2c","2ebd8498d5814fc0b8271f0b2ebf143f","b1d1662e6cfa43ada79e58126b0be16f","c1d3d8118655447aa4154fc5a351dc56","fc26a949fbdf4d4fb2c2eaeb73ff4423","425ff7a8d163478cb614ed1af4ed68b2","1e44c975ba7040ff905f6da678700d0a","4a696f3c77a9484c82f5e29ebd29b80a","dee73fafd21d4a928e0d31acd2de7b70"]},"id":"c0d6-_2qzTKW","outputId":"14badfb6-c376-4484-c70e-ed8616bf1816","executionInfo":{"status":"ok","timestamp":1711687060303,"user_tz":300,"elapsed":28250,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a67081ba3146eda7e386f76c375477"}},"metadata":{}}],"source":["SB_ControlNet_pipeline = StableDiffusionControlnetPipeline()"]},{"cell_type":"markdown","metadata":{"id":"HoLKx9z_5547"},"source":["# **Prompt**"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"XIPnazdo3vGx","executionInfo":{"status":"ok","timestamp":1711687176842,"user_tz":300,"elapsed":147,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["def merge_prompts(prompts):\n","  return \", \".join(prompts)"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"-rPBFkHzh3j3","executionInfo":{"status":"ok","timestamp":1711687176937,"user_tz":300,"elapsed":1,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["# @markdown #**Positive Prompt**\n","\n","PROMPT_1 = \"(change the color of the hair roots to be like the rest of the hair color:1.2)\" # @param {type:\"string\"}\n","PROMPT_2 = \"raw photo, high detail, best quality, keep the same style of the hair\" # @param {type:\"string\"}\n","PROMPT_3 = \"beautiful hair, beautiful hair roots\" # @param {type:\"string\"}\n","prompt_list = [PROMPT_1, PROMPT_2, PROMPT_3]\n","PROMPT = merge_prompts(prompt_list)\n","# @markdown #**Negative Prompt**\n","\n","NEG_PROMPT_1 =  \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime)\" # @param {type:\"string\"}\n","NEG_PROMPT_2 = \"text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers\" # @param {type:\"string\"}\n","NEG_PROMPT_3 = \"mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions\" # @param {type:\"string\"}\n","NEG_PROMPT_4 = \"extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs\"# @param {type:\"string\"}\n","NEG_PROMPT_5 = \"fused fingers, too many fingers, long neck\" # @param {type:\"string\"}\n","negative_prompt_list = [NEG_PROMPT_1, NEG_PROMPT_2, NEG_PROMPT_3, NEG_PROMPT_4, NEG_PROMPT_5]\n","NEGATIVE_PROMPT = merge_prompts(negative_prompt_list)\n"]},{"cell_type":"markdown","metadata":{"id":"0B5l3a30G-M1"},"source":["# **Touch up the hair for a given list of images**"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxZ93Q0JXPjZ","outputId":"0780d011-807d-4ba8-acf8-92907a1abbb8","executionInfo":{"status":"ok","timestamp":1711691763838,"user_tz":300,"elapsed":7563,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:07<00:00,  1.48s/it]\n"]}],"source":["# Generate minority and majority color hair masks\n","demo_img_list = [1, 19, 21, 22, 32]\n","if __name__ == \"__main__\":\n","  for i in tqdm(demo_img_list):\n","    # Get image path\n","    IMG_PATH = os.path.join(project_dir_path,\"input_dir/Images\", \"pic\" + str(i) + \".png\")\n","\n","    # Generate masks\n","    minority_hair_mask = GenerateMask(IMG_PATH).get_minority_hair_mask()\n","    majority_hair_mask = GenerateMask(IMG_PATH).get_minority_hair_mask(majority=True)\n","\n","    # Create minority and majority hair mask paths\n","    MINORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, minority_color_mask_dir_path,\"pic\" + str(i) + \"minority_color_mask.png\")\n","    MAJORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, majority_color_mask_dir_path,\"pic\" + str(i) + \"majority_color_mask.png\")\n","\n","    # Save the results\n","    cv2.imwrite(MINORITY_HAIR_MASK_PATH, minority_hair_mask)\n","    cv2.imwrite(MAJORITY_HAIR_MASK_PATH, majority_hair_mask)"]},{"cell_type":"code","source":["def fit_pipeline(img_path):\n","  return 0"],"metadata":{"id":"RCQgJ0RJyh0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NOTE: You must upload image (or folder of images, in this case) to the 'input_dir' before running this function!\n","\n","#demo_img_list = [1,19,21,22,32]\n","demo_img_list = [1]\n","if __name__ == \"__main__\":\n","  for i in tqdm(demo_img_list):\n","    # In this case, images are stored in an 'Images' folder placed in the 'input_dir' with names like 'pic1.png', 'pic2.png', etc.\n","    IMG_PATH = os.path.join(project_dir_path,\"input_dir/Images\", \"pic\" + str(i) + \".png\")\n","    MINORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, minority_color_mask_dir_path,\"pic\" + str(i) + \"minority_color_mask.png\")\n","    MAJORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, majority_color_mask_dir_path,\"pic\" + str(i) + \"majority_color_mask.png\")\n","\n","    # Process the image with the pipeline\n","    touched_image = stable_diffusion_controlnet(self, image_path, mask_image, PROMPT, NEGATIVE_PROMPT, dilate=False, num_inference_steps=50, strength=1.0, guidance_scale=4.0, controlnet_inpaint_scale=0.5, controlnet_canny_scale=0.2):\n","    touched_image_restored = face_restoration(touched_image) # Restore face as seperate function??!\n","\n","    # Save the results\n","    TOUCHED_IMAGE_PATH = os.path.join(project_dir_path, touched_dir_path, \"pic\" + str(i) + \"_touched.png\")\n","    RESTORED_IMAGE_PATH = os.path.join(project_dir_path, restored_dir_path, \"pic\" + str(i) + \"_restored.png\")\n","\n","    cv2.imwrite(MINORITY_HAIR_MASK_PATH, minority_hair_mask)\n","    touched_image.save(TOUCHED_IMAGE_PATH)\n","    touched_image_restored.save(RESTORED_IMAGE_PATH)"],"metadata":{"id":"6d0hGDUHxYMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGBT5Jo0L6hD","executionInfo":{"status":"aborted","timestamp":1711676931160,"user_tz":300,"elapsed":791889,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["# def apply_face_mask_overlay(image, face_mask, color = (0, 255, 255), alpha = 1.0):\n","#     pil_image = image.convert('RGB')\n","#     open_cv_image = np.array(pil_image)\n","#     # Convert RGB to BGR\n","#     input_image = open_cv_image[:, :, ::-1].copy()\n","#     # Load the input image and mask\n","#     gray_mask = cv2.cvtColor(face_mask, cv2.COLOR_BGR2GRAY)\n","#     mask_image = cv2.resize(gray_mask, input_image.shape[:2])\n","\n","#     # Create a blank image with the same dimensions as the input image\n","#     overlay = np.zeros_like(input_image)\n","\n","#     # Apply the overlay color to the blank image using the mask\n","#     overlay[mask_image > 0] = color\n","\n","#     result = input_image.copy()\n","#     # Combine the input image and the overlay using alpha blending\n","#     result[mask_image > 0] = cv2.addWeighted(input_image[mask_image > 0], 0.1, overlay[mask_image > 0], alpha, 0)\n","\n","\n","#     # Convert the resulting image from OpenCV to PIL format\n","#     result_pil = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n","\n","#     return result_pil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9TmLFJuNQ0P","executionInfo":{"status":"aborted","timestamp":1711676931160,"user_tz":300,"elapsed":791307,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"outputs":[],"source":["  # demo_img_list = [1,19,21,22,32]\n","  # ReSize = (512,512)\n","  # img_titles = [\"Original Image\", \"Touched Image\", \"Restored Image\"]\n","  # if __name__ == \"__main__\":\n","  #   for n, i in enumerate(demo_img_list):\n","  #     fig, axes = plt.subplots(1, 3, figsize = (16,9))\n","  #     IMG_PATH =  os.path.join(project_dir_path,\"input_dir\", \"pic\" + str(i) + \".png\")\n","  #     MINORITY_HAIR_MASK_PATH = os.path.join(project_dir_path, mask_dir_path,\"pic\" + str(i) + \"_mask.png\")\n","  #     TOUCHED_IMAGE_PATH = os.path.join(project_dir_path, touched_dir_path, \"pic\" + str(i) + \"_touched.png\")\n","  #     RESTORED_IMAGE_PATH = os.path.join(project_dir_path, restored_dir_path, \"pic\" + str(i) + \"_restored.png\")\n","  #     mask_generater = GenerateMask(IMG_PATH)\n","  #     face_mask = mask_generater.get_face_mask()\n","  #     image = Image.open(IMG_PATH)\n","  #     touched_image = Image.open(TOUCHED_IMAGE_PATH)\n","  #     restored_image = Image.open(RESTORED_IMAGE_PATH)\n","  #     img_list = [image.resize(ReSize), touched_image.resize(ReSize), restored_image.resize(ReSize)]\n","  #     for j, img_ in enumerate(img_list):\n","  #         img = apply_face_mask_overlay(img_,face_mask)\n","\n","  #         axes[j].imshow(img)\n","  #         axes[j].axis('off')\n","  #         if n == 0:\n","  #           axes[j].set_title(img_titles[j])\n","\n","  #     plt.show()\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"761MUyqB2Skz","executionInfo":{"status":"aborted","timestamp":1711676931161,"user_tz":300,"elapsed":790838,"user":{"displayName":"Michael Tornaritis","userId":"03443415642242315369"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/fellowship/touch-up-the-hair/blob/main/touch_up_the_hair_experiementation.ipynb","timestamp":1711418707917}],"collapsed_sections":["6RQZ7gDe6h6q"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"80a67081ba3146eda7e386f76c375477":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46959835013e4ff888fa8c980bb63b2f","IPY_MODEL_272e61cde96447abad606cbba9781c2c","IPY_MODEL_2ebd8498d5814fc0b8271f0b2ebf143f"],"layout":"IPY_MODEL_b1d1662e6cfa43ada79e58126b0be16f"}},"46959835013e4ff888fa8c980bb63b2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1d3d8118655447aa4154fc5a351dc56","placeholder":"​","style":"IPY_MODEL_fc26a949fbdf4d4fb2c2eaeb73ff4423","value":"Loading pipeline components...: 100%"}},"272e61cde96447abad606cbba9781c2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_425ff7a8d163478cb614ed1af4ed68b2","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e44c975ba7040ff905f6da678700d0a","value":6}},"2ebd8498d5814fc0b8271f0b2ebf143f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a696f3c77a9484c82f5e29ebd29b80a","placeholder":"​","style":"IPY_MODEL_dee73fafd21d4a928e0d31acd2de7b70","value":" 6/6 [00:01&lt;00:00,  3.80it/s]"}},"b1d1662e6cfa43ada79e58126b0be16f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1d3d8118655447aa4154fc5a351dc56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc26a949fbdf4d4fb2c2eaeb73ff4423":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"425ff7a8d163478cb614ed1af4ed68b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e44c975ba7040ff905f6da678700d0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a696f3c77a9484c82f5e29ebd29b80a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dee73fafd21d4a928e0d31acd2de7b70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}